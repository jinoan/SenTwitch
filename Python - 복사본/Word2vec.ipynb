{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec를 이용하기 위해 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def tokenizer(line, okt, remove_stopwords = False, stop_words = []):\n",
    "    # line: 전처리할 텍스트\n",
    "    # okt: okt 객체를 반복적으로 생성하지 않고 미리 생성한 후 인자로 받는다.\n",
    "    # remove_stopword: 불용어를 제거할지 여부 선택, 기본값은 False\n",
    "    # stop_word: 불용어 사전은 사용자가 직접 입력해야 함. 기본값은 빈 리스트\n",
    "\n",
    "    # okt 객체를 활용해 형태소 단위로 나눈다.\n",
    "    word_token = okt.morphs(line, norm=True, stem=False)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        # 불용어 제거(선택적)\n",
    "        word_review = [token for token in word_token if not token in stop_words]\n",
    "    \n",
    "    return word_token\n",
    "\n",
    "from string import ascii_letters\n",
    "def char_3x_shift(st):\n",
    "    # 한글 자음 리스트\n",
    "    CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    # 한글 모음 리스트\n",
    "    JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ']\n",
    "    # 알파벳 리스트\n",
    "    ALPHABET_LIST = list(ascii_letters)\n",
    "    # 구두기호 리스트 (\\ 제외)\n",
    "    PUNCTUATION_LIST = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':',\n",
    "                        ';', '<', '=', '>', '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "    \n",
    "    \n",
    "    CHAR_LIST = [CHOSUNG_LIST, JUNGSUNG_LIST, ALPHABET_LIST]\n",
    "    \n",
    "    for l in CHAR_LIST:\n",
    "        for c in l:\n",
    "            st = re.sub(c+'{3,}', c*3, st)\n",
    "            \n",
    "    for p in PUNCTUATION_LIST:\n",
    "        st = re.sub('\\\\'+p+'{3,}', p*3, st)\n",
    "    \n",
    "    return st\n",
    "\n",
    "def strip_e(st):\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    return RE_EMOJI.sub(r'', st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 채팅부분 토크나이징\n",
    "import json\n",
    "\n",
    "DATA_IN_PATH = './data/video_chat/'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + 'chat_log.csv')\n",
    "tokenized_train_data = []\n",
    "\n",
    "print('tokenizer start!')\n",
    "\n",
    "batch_size = 10000\n",
    "epoch_size = int(train_data.shape[0] / batch_size)\n",
    "remainder = train_data.shape[0] % batch_size\n",
    "\n",
    "for idx, line in enumerate(train_data['chat']):\n",
    "    tokenized_train_data.append(tokenizer(strip_e(char_3x_shift(line)), okt))\n",
    "    if idx % 10000 == 0:\n",
    "        print('{} / {}'.format(idx, train_data.shape[0]))\n",
    "\n",
    "print('tokenized_train_data_size: {}'.format(len(tokenized_train_data)))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 앞뒤 3줄씩 합치기\n",
    "list_size = len(tokenized_train_data[:-2])\n",
    "for idx, item in enumerate(tokenized_train_data[:-2]):\n",
    "    tokenized_train_data[idx] = tokenized_train_data[idx] + tokenized_train_data[idx+1] + tokenized_train_data[idx+2]\n",
    "    if idx % 10000 == 0:\n",
    "        print('{} / {}'.format(idx, list_size))\n",
    "clean_train_data = tokenized_train_data[:-2]\n",
    "print(clean_train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 채팅 json 형식으로 저장\n",
    "DATA_IN_PATH = './data/video_chat/'\n",
    "with open(DATA_IN_PATH + 'chat_token.json',\n",
    "          'w', encoding='utf-8') as file:\n",
    "    try:\n",
    "        json.dump(clean_train_data, file, ensure_ascii=False, indent=\"\\t\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vc 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 채팅 파일 가져오기\n",
    "import json\n",
    "DATA_IN_PATH = './data/video_chat/'\n",
    "with open(DATA_IN_PATH + 'chat_token.json', 'r', encoding='utf-8') as file:\n",
    "    clean_train_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시 필요한 하이퍼 파라미터\n",
    "num_features = 150  # 워드 벡터 특징값 수\n",
    "min_word_count = 30  # 단어에 대한 최소 빈도 수\n",
    "num_workers = 4  # 프로세스 개수\n",
    "context = 5  # 컨텍스트 윈도우 크기\n",
    "downsampling = 1e-3  # 다운 샘플링 비율\n",
    "sg = 1  # 스킵그램 사용 여부\n",
    "hs = 0  # 0: negative sampling, 1: hierarchical softmax\n",
    "negative = 5  # negative sampling 할 노이즈 단어의 수\n",
    "#batch_words_size = 30  # 배치 단어의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 학습 진행상황을 확인할 수 있도록 logging 설정\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word2vec 학습\n",
    "from gensim.models import word2vec\n",
    "print('Training model...')\n",
    "model = word2vec.Word2Vec(clean_train_data,\n",
    "                         workers=num_workers,\n",
    "                         size=num_features,\n",
    "                         min_count=min_word_count,\n",
    "                         window=context,\n",
    "                         sample=downsampling,\n",
    "                         sg = sg,\n",
    "                         hs = hs,\n",
    "                         negative = negative\n",
    "                         #batch_words=batch_words_size\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담는다면 나중에 참고하기 좋을 것이다.\n",
    "# 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있다.\n",
    "model_name = '150features_30minwords_5context'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "from gensim.models import word2vec\n",
    "model_name = '150features_30minwords_5context'\n",
    "model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec 벡터 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=[\"ㅋㅋㅋ\"], topn=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
